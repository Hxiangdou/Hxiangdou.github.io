

<!DOCTYPE html>
<html lang="en" color-mode=light>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>动手学习深度学习（12）现代卷积神经网络 - Hexo</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />
  
  <meta name="description" content="本章我们将带你了解现代的卷积神经网络架构，在本章中的每...">
  <meta name="author" content="茴香豆">
  <link rel="icon" href="/images/icons/favicon-16x16.png" type="image/png" sizes="16x16">
  <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="32x32">
  <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
    
<link rel="stylesheet" href="https://at.alicdn.com/t/font_1445822_p6ry5n7lrr.css">

  

  
    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css">

  

  
    
      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/xcode.min.css" name="highlight-style" mode="light">

      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/solarized-dark.min.css" name="highlight-style" mode="dark">

      
  

  <script>
    var CONFIG = window.CONFIG || {};
    var ZHAOO = window.ZHAOO || {};
    CONFIG = {
      isHome: false,
      fancybox: true,
      pjax: false,
      loading: {
        gif: '/images/theme/loading.gif',
        lottie: ''
      },
      lazyload: {
        enable: true,
        only_post: 'false',
        loading: {
          gif: '/images/theme/loading.gif',
          lottie: ''
        }
      },
      donate: {
        enable: true,
        alipay: '/images/支付宝.JPG',
        wechat: '/images/微信.JPG'
      },
      galleries: {
        enable: true
      },
      fab: {
        enable: true,
        always_show: false
      },
      carrier: {
        enable: true
      },
      daovoice: {
        enable: true
      },
      preview: {
        background: {
          default: '',
          api: ''
        },
        motto: {
          default: '我在开了灯的床头下，想问问自己的心啊。',
          typing: true,
          api: 'https://v2.jinrishici.com/one.json',
          data_contents: '["data","content"]'
        },
      },
      qrcode: {
        enable: true,
        type: 'url',
        image: 'https://pic.izhaoo.com/weapp-code.jpg',
      },
      toc: {
        enable: true
      },
      scrollbar: {
        type: 'default'
      },
      notification: {
        enable: false,
        delay: 4500,
        list: '',
        page_white_list: '',
        page_black_list: ''
      },
      search: {
        enable: true,
        path: '/search.xml'
      }
    }
  </script>

  

  

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head>

<body class="lock-screen">
  <div class="loading" id="loading"></div>
  
    


  <nav class="navbar">
    <div class="left">
      
        <i class="iconfont iconhome j-navbar-back-home"></i>
      
      
        <i class="iconfont iconqrcode j-navbar-qrcode"></i>
      
      
        <i class="iconfont iconmoono" id="color-toggle" color-toggle="light"></i>
      
      
        <i class="iconfont iconsearch j-navbar-search"></i>
      
    </div>
    <div class="center">动手学习深度学习（12）现代卷积神经网络</div>
    <div class="right">
      <i class="iconfont iconmenu j-navbar-menu"></i>
    </div>
    
      <div id="qrcode-navbar"></div>
    
  </nav>

  
  

<nav class="menu">
  <div class="menu-container">
    <div class="menu-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <ul class="menu-content"><li class="menu-item">
        <a href="/ " class="underline "> 首页</a>
      </li><li class="menu-item">
        <a href="/galleries/ " class="underline "> 相册</a>
      </li><li class="menu-item">
        <a href="/archives/ " class="underline "> 归档</a>
      </li><li class="menu-item">
        <a href="/tags/ " class="underline "> 标签</a>
      </li><li class="menu-item">
        <a href="/categories/ " class="underline "> 分类</a>
      </li><li class="menu-item">
        <a href="/about/ " class="underline "> 关于</a>
      </li></ul>
    
      <div class="menu-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Theme - <a target="_blank" href="https://github.com/izhaoo/hexo-theme-zhaoo">zhaoo</a></p></div>
    
  </div>
</nav>
  <main id="main">
  <div class="article-wrap">
    <div class="row container">
      <div class="col-xl-3"></div>
      <div class="col-xl-6"><article class="article">
  <div class="wrap">
    <section class="head">
  <img   class="lazyload" data-original="/images/pat/2.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">动手学习深度学习（12）现代卷积神经网络</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>October 31, 2022</span>
      
        <span class="post-info-item">
          <i class="iconfont iconeye"></i><span id="/2022/10/31/DL_12/" class="leancloud-counter" data-flag-title="动手学习深度学习（12）现代卷积神经网络"></span>
        </span>
        <span class="post-info-item">
          <i class="iconfont iconheart"></i><span id="/2022/10/31/DL_12/" class="leancloud-like" data-flag-title="动手学习深度学习（12）现代卷积神经网络"></span>
        </span>
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>10746</span>
    </div>
  </div>
</section>
    <section class="main">
      <section class="content">
        
        <p>本章我们将带你了解现代的卷积神经网络架构，在本章中的每一个模型都曾一度占据主导地位，其中许多模型都是ImageNet竞赛的优胜者。</p>
<h2 id="AlexNet"><a href="#AlexNet" class="headerlink" title="AlexNet"></a>AlexNet</h2><p>是一个更深更大的LeNet，主要改进：dropout、ReLu、MaxPooling</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br>net = nn.Sequential(<br>    <span class="hljs-comment"># 这里，我们使用一个11*11的更大窗口来捕捉对象。</span><br>    <span class="hljs-comment"># 同时，步幅为4，以减少输出的高度和宽度。</span><br>    <span class="hljs-comment"># 另外，输出通道的数目远大于LeNet</span><br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, kernel_size=<span class="hljs-number">11</span>, stride=<span class="hljs-number">4</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># 减小卷积窗口，使用填充为2来使得输入与输出的高和宽一致，且增大输出通道数</span><br>    nn.Conv2d(<span class="hljs-number">96</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    <span class="hljs-comment"># 使用三个连续的卷积层和较小的卷积窗口。</span><br>    <span class="hljs-comment"># 除了最后的卷积层，输出通道的数量进一步增加。</span><br>    <span class="hljs-comment"># 在前两个卷积层之后，汇聚层不用于减少输入的高度和宽度</span><br>    nn.Conv2d(<span class="hljs-number">256</span>, <span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">384</span>, <span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.Conv2d(<span class="hljs-number">384</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>), nn.ReLU(),<br>    nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Flatten(),<br>    <span class="hljs-comment"># 这里，全连接层的输出数量是LeNet中的好几倍。使用dropout层来减轻过拟合</span><br>    nn.Linear(<span class="hljs-number">6400</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>    nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>    nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>), nn.ReLU(),<br>    nn.Dropout(p=<span class="hljs-number">0.5</span>),<br>    <span class="hljs-comment"># 最后是输出层。由于这里使用Fashion-MNIST，所以用类别数为10，而非论文中的1000</span><br>    nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>))<br><span class="hljs-comment"># 构造一个高度和宽度都为224的单通道数据，来观察每一层输出的形状。</span><br>X = torch.randn(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>)<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X=layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>,X.shape)<br><span class="hljs-comment"># output</span><br>Conv2d output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">54</span>, <span class="hljs-number">54</span>])<br>ReLU output shape:   torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">54</span>, <span class="hljs-number">54</span>])<br>MaxPool2d output shape:      torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">26</span>, <span class="hljs-number">26</span>])<br>Conv2d output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">26</span>, <span class="hljs-number">26</span>])<br>ReLU output shape:   torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">26</span>, <span class="hljs-number">26</span>])<br>MaxPool2d output shape:      torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>Conv2d output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>ReLU output shape:   torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>Conv2d output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>ReLU output shape:   torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>Conv2d output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>ReLU output shape:   torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>MaxPool2d output shape:      torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br>Flatten output shape:        torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">6400</span>])<br>Linear output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>ReLU output shape:   torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Dropout output shape:        torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Linear output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>ReLU output shape:   torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Dropout output shape:        torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Linear output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 读取数据集</span><br>batch_size = <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">224</span>)<br><span class="hljs-comment"># 训练AlexNet</span><br>lr, num_epochs = <span class="hljs-number">0.01</span>, <span class="hljs-number">10</span><br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br><span class="hljs-comment"># output</span><br>loss <span class="hljs-number">0.327</span>, train acc <span class="hljs-number">0.881</span>, test acc <span class="hljs-number">0.885</span><br><span class="hljs-number">4149.6</span> examples/sec on cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>

<h2 id="VGG-使用块的网络"><a href="#VGG-使用块的网络" class="headerlink" title="VGG 使用块的网络"></a>VGG 使用块的网络</h2><p>更大更深的AlexNet（重复的VGG块）</p>
<ul>
<li>VGG使用可重复使用的卷积块来构建深度卷积神经网络</li>
<li>不同的卷积块个数和超参数可以得到不同复杂度的变种</li>
</ul>
<p>VGG块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vgg_block</span>(<span class="hljs-params">num_convs, in_channels, out_channels</span>):<br>    layers = []<br>    <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_convs):<br>        layers.append(nn.Conv2d(in_channels, out_channels,<br>                                kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>))<br>        layers.append(nn.ReLU())<br>        in_channels = out_channels<br>    layers.append(nn.MaxPool2d(kernel_size=<span class="hljs-number">2</span>,stride=<span class="hljs-number">2</span>))<br>    <span class="hljs-keyword">return</span> nn.Sequential(*layers)<br></code></pre></td></tr></table></figure>

<p>VGG网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><code class="hljs python">conv_arch = ((<span class="hljs-number">1</span>, <span class="hljs-number">64</span>), (<span class="hljs-number">1</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>), (<span class="hljs-number">2</span>, <span class="hljs-number">512</span>))<br><span class="hljs-keyword">def</span> <span class="hljs-title function_">vgg</span>(<span class="hljs-params">conv_arch</span>):<br>    conv_blks = []<br>    in_channels = <span class="hljs-number">1</span><br>    <span class="hljs-comment"># 卷积层部分</span><br>    <span class="hljs-keyword">for</span> (num_convs, out_channels) <span class="hljs-keyword">in</span> conv_arch:<br>        conv_blks.append(vgg_block(num_convs, in_channels, out_channels))<br>        in_channels = out_channels<br><br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        *conv_blks, nn.Flatten(),<br>        <span class="hljs-comment"># 全连接层部分</span><br>        nn.Linear(out_channels * <span class="hljs-number">7</span> * <span class="hljs-number">7</span>, <span class="hljs-number">4096</span>), nn.ReLU(), nn.Dropout(<span class="hljs-number">0.5</span>),<br>        nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">4096</span>), nn.ReLU(), nn.Dropout(<span class="hljs-number">0.5</span>),<br>        nn.Linear(<span class="hljs-number">4096</span>, <span class="hljs-number">10</span>))<br><br>net = vgg(conv_arch)<br><span class="hljs-comment"># 构建一个高度和宽度为224的单通道数据样本，以观察每个层输出的形状。</span><br>X = torch.randn(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br><span class="hljs-keyword">for</span> blk <span class="hljs-keyword">in</span> net:<br>    X = blk(X)<br>    <span class="hljs-built_in">print</span>(blk.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>,X.shape)<br><span class="hljs-comment"># output</span><br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">112</span>, <span class="hljs-number">112</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">56</span>, <span class="hljs-number">56</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>])<br>Flatten output shape:        torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">25088</span>])<br>Linear output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>ReLU output shape:   torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Dropout output shape:        torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Linear output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>ReLU output shape:   torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Dropout output shape:        torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">4096</span>])<br>Linear output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure>

<p>构建一个通道数较少的网络，足够用于训练Fashion-MNIST数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><code class="hljs python">ratio = <span class="hljs-number">4</span><br>small_conv_arch = [(pair[<span class="hljs-number">0</span>], pair[<span class="hljs-number">1</span>] // ratio) <span class="hljs-keyword">for</span> pair <span class="hljs-keyword">in</span> conv_arch]<br>net = vgg(small_conv_arch)<br>lr, num_epochs, batch_size = <span class="hljs-number">0.05</span>, <span class="hljs-number">10</span>, <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">224</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br><span class="hljs-comment"># output</span><br>loss <span class="hljs-number">0.177</span>, train acc <span class="hljs-number">0.934</span>, test acc <span class="hljs-number">0.911</span><br><span class="hljs-number">2562.3</span> examples/sec on cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>

<h2 id="NiN-网络中的网络"><a href="#NiN-网络中的网络" class="headerlink" title="NiN 网络中的网络"></a>NiN 网络中的网络</h2><p>NiN块：一个卷积层后跟两个全连接层</p>
<ul>
<li>步幅1， 无填充，输出形状跟卷积层输出一样</li>
<li>起到全连接层的作用</li>
</ul>
<p>NiN架构：</p>
<ul>
<li>无全连接层</li>
<li>交替使用NiN块和步幅为2的MaxPooling层<ul>
<li>逐步减小高宽和增大通道数</li>
</ul>
</li>
<li>最后使用全局平均池化层得到输出<ul>
<li>其输入通道数是类别数</li>
</ul>
</li>
</ul>
<p>NiN使用全局平均池化层来替代VGG和AlexNet中的全连接层</p>
<ul>
<li>不容易过拟合，更少的参数个数</li>
</ul>
<p>NiN块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">nin_block</span>(<span class="hljs-params">in_channels, out_channels, kernel_size, strides, padding</span>):<br>    <span class="hljs-keyword">return</span> nn.Sequential(<br>        nn.Conv2d(in_channels, out_channels, kernel_size, strides, padding),<br>        nn.ReLU(),<br>        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="hljs-number">1</span>), nn.ReLU(),<br>        nn.Conv2d(out_channels, out_channels, kernel_size=<span class="hljs-number">1</span>), nn.ReLU())<br></code></pre></td></tr></table></figure>

<p>NiN模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python">net = nn.Sequential(<br>    nin_block(<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, kernel_size=<span class="hljs-number">11</span>, strides=<span class="hljs-number">4</span>, padding=<span class="hljs-number">0</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    nin_block(<span class="hljs-number">96</span>, <span class="hljs-number">256</span>, kernel_size=<span class="hljs-number">5</span>, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-number">2</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    nin_block(<span class="hljs-number">256</span>, <span class="hljs-number">384</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>    nn.MaxPool2d(<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Dropout(<span class="hljs-number">0.5</span>),<br>    <span class="hljs-comment"># 标签类别数是10</span><br>    nin_block(<span class="hljs-number">384</span>, <span class="hljs-number">10</span>, kernel_size=<span class="hljs-number">3</span>, strides=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>),<br>    nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>, <span class="hljs-number">1</span>)),<br>    <span class="hljs-comment"># 将四维的输出转成二维的输出，其形状为(批量大小,10)</span><br>    nn.Flatten())<br><span class="hljs-comment"># 查看每个块的形状</span><br>X = torch.rand(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X = layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br><span class="hljs-comment"># output</span><br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">54</span>, <span class="hljs-number">54</span>])<br>MaxPool2d output shape:      torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">26</span>, <span class="hljs-number">26</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">26</span>, <span class="hljs-number">26</span>])<br>MaxPool2d output shape:      torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>MaxPool2d output shape:      torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br>Dropout output shape:        torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">384</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">5</span>, <span class="hljs-number">5</span>])<br>AdaptiveAvgPool2d output shape:      torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>Flatten output shape:        torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 训练模型</span><br>lr, num_epochs, batch_size = <span class="hljs-number">0.1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">224</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br><span class="hljs-comment"># output</span><br>loss <span class="hljs-number">0.363</span>, train acc <span class="hljs-number">0.865</span>, test acc <span class="hljs-number">0.879</span><br><span class="hljs-number">3212.2</span> examples/sec on cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>

<h2 id="GoogLeNet"><a href="#GoogLeNet" class="headerlink" title="GoogLeNet"></a>GoogLeNet</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1b5411g7Xo/?spm_id_from=autoNext&vd_source=f1e7eb1d150afc7b732a2b8c557e6d35">GoogLeNet &#x2F; Inception V3【动手学深度学习v2】</a></p>
<p>Inception块：4个路径从不同层面抽取信息，然后再输出通道合并</p>
<ul>
<li>优点是模型参数小，计算复杂度低</li>
</ul>
<p>GoogLeNet使用了9个Inception块</p>
<p>实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Inception</span>(nn.Module):<br>    <span class="hljs-comment"># c1--c4是每条路径的输出通道数</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, in_channels, c1, c2, c3, c4, **kwargs</span>):<br>        <span class="hljs-built_in">super</span>(Inception, self).__init__(**kwargs)<br>        <span class="hljs-comment"># 线路1，单1x1卷积层</span><br>        self.p1_1 = nn.Conv2d(in_channels, c1, kernel_size=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 线路2，1x1卷积层后接3x3卷积层</span><br>        self.p2_1 = nn.Conv2d(in_channels, c2[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p2_2 = nn.Conv2d(c2[<span class="hljs-number">0</span>], c2[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 线路3，1x1卷积层后接5x5卷积层</span><br>        self.p3_1 = nn.Conv2d(in_channels, c3[<span class="hljs-number">0</span>], kernel_size=<span class="hljs-number">1</span>)<br>        self.p3_2 = nn.Conv2d(c3[<span class="hljs-number">0</span>], c3[<span class="hljs-number">1</span>], kernel_size=<span class="hljs-number">5</span>, padding=<span class="hljs-number">2</span>)<br>        <span class="hljs-comment"># 线路4，3x3最大汇聚层后接1x1卷积层</span><br>        self.p4_1 = nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">1</span>, padding=<span class="hljs-number">1</span>)<br>        self.p4_2 = nn.Conv2d(in_channels, c4, kernel_size=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, x</span>):<br>        p1 = F.relu(self.p1_1(x))<br>        p2 = F.relu(self.p2_2(F.relu(self.p2_1(x))))<br>        p3 = F.relu(self.p3_2(F.relu(self.p3_1(x))))<br>        p4 = F.relu(self.p4_2(self.p4_1(x)))<br>        <span class="hljs-comment"># 在通道维度上连结输出</span><br>        <span class="hljs-keyword">return</span> torch.cat((p1, p2, p3, p4), dim=<span class="hljs-number">1</span>)<br></code></pre></td></tr></table></figure>

<p>接下来逐一实现GoogLeNet的每个模块</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-comment"># 第一个模块</span><br>b1 = nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                   nn.ReLU(),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><span class="hljs-comment"># 第二个模块</span><br>b2 = nn.Sequential(nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">1</span>),<br>                   nn.ReLU(),<br>                   nn.Conv2d(<span class="hljs-number">64</span>, <span class="hljs-number">192</span>, kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>),<br>                   nn.ReLU(),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><span class="hljs-comment"># 第三个模块</span><br>b3 = nn.Sequential(Inception(<span class="hljs-number">192</span>, <span class="hljs-number">64</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">128</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">32</span>), <span class="hljs-number">32</span>),<br>                   Inception(<span class="hljs-number">256</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">192</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">96</span>), <span class="hljs-number">64</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><span class="hljs-comment"># 第四个模块</span><br>b4 = nn.Sequential(Inception(<span class="hljs-number">480</span>, <span class="hljs-number">192</span>, (<span class="hljs-number">96</span>, <span class="hljs-number">208</span>), (<span class="hljs-number">16</span>, <span class="hljs-number">48</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">160</span>, (<span class="hljs-number">112</span>, <span class="hljs-number">224</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">128</span>, (<span class="hljs-number">128</span>, <span class="hljs-number">256</span>), (<span class="hljs-number">24</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">512</span>, <span class="hljs-number">112</span>, (<span class="hljs-number">144</span>, <span class="hljs-number">288</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">64</span>), <span class="hljs-number">64</span>),<br>                   Inception(<span class="hljs-number">528</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><span class="hljs-comment"># 第五个模块</span><br>b5 = nn.Sequential(Inception(<span class="hljs-number">832</span>, <span class="hljs-number">256</span>, (<span class="hljs-number">160</span>, <span class="hljs-number">320</span>), (<span class="hljs-number">32</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   Inception(<span class="hljs-number">832</span>, <span class="hljs-number">384</span>, (<span class="hljs-number">192</span>, <span class="hljs-number">384</span>), (<span class="hljs-number">48</span>, <span class="hljs-number">128</span>), <span class="hljs-number">128</span>),<br>                   nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                   nn.Flatten())<br><br>net = nn.Sequential(b1, b2, b3, b4, b5, nn.Linear(<span class="hljs-number">1024</span>, <span class="hljs-number">10</span>))<br><span class="hljs-comment"># 下面演示各个模块输出的形状变化</span><br>X = torch.rand(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">96</span>, <span class="hljs-number">96</span>))<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X = layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br><span class="hljs-comment"># output</span><br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">24</span>, <span class="hljs-number">24</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">192</span>, <span class="hljs-number">12</span>, <span class="hljs-number">12</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">480</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">832</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">1024</span>])<br>Linear output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure>

<p>训练模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">0.1</span>, <span class="hljs-number">10</span>, <span class="hljs-number">128</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">96</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br><span class="hljs-comment"># output</span><br>loss <span class="hljs-number">0.254</span>, train acc <span class="hljs-number">0.904</span>, test acc <span class="hljs-number">0.885</span><br><span class="hljs-number">3570.5</span> examples/sec on cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>


      </section>
      <section class="extra">
        
          <ul class="copyright">
  
    <li><strong>本文作者：</strong>茴香豆</li>
    <li><strong>本文链接：</strong><a href="https://hxiangdou.github.io/2022/10/31/DL_12/index.html" title="https:&#x2F;&#x2F;hxiangdou.github.io&#x2F;2022&#x2F;10&#x2F;31&#x2F;DL_12&#x2F;index.html">https:&#x2F;&#x2F;hxiangdou.github.io&#x2F;2022&#x2F;10&#x2F;31&#x2F;DL_12&#x2F;index.html</a></li>
    <li><strong>版权声明：</strong>本博客所有文章均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" title="BY-NC-SA" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请注明出处！</li>
  
</ul>
        
        
          <section class="donate">
  <div id="qrcode-donate">
    <img   class="lazyload" data-original="/images/支付宝.JPG" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" >
  </div>
  <div class="icon">
    <a href="javascript:;" id="alipay"><i class="iconfont iconalipay"></i></a>
    <a href="javascript:;" id="wechat"><i class="iconfont iconwechat-fill"></i></a>
  </div>
</section>
        
        
  <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/" rel="tag">DeepLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li></ul> 

        
  <nav class="nav">
    <a></a>
    <a href="/2022/10/31/DL_11/">动手学习深度学习（11）LeNet<i class="iconfont iconright"></i></a>
  </nav>

      </section>
      
        <section class="comments">
  
    <div class="btn" id="comments-btn">查看评论</div>
  
  
<div id="valine"></div>
<script defer src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
  window.onload = function () {
    var loadValine = function () {
      new Valine({
        el: '#valine',
        app_id: "eSrfpdL3XjSFRaFUwnuRl3dn-gzGzoHsz",
        app_key: "6DvkgciWlfleRrT7w9r6snJI",
        placeholder: "雁过留痕",
        avatar: "mp",
        pageSize: "10",
        lang: "zh-CN",
      });
    }
    if ( true ) {
      $("#comments-btn").on("click", function () {
        $(this).hide();
        loadValine();
      });
    } else {
      loadValine();
    }
  };
</script>

</section>
      
    </section>
  </div>
</article></div>
      <div class="col-xl-3">
        
          
  <aside class="toc-wrap">
    <h3 class="toc-title">文章目录：</h3>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#AlexNet"><span class="toc-text">AlexNet</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VGG-%E4%BD%BF%E7%94%A8%E5%9D%97%E7%9A%84%E7%BD%91%E7%BB%9C"><span class="toc-text">VGG 使用块的网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#NiN-%E7%BD%91%E7%BB%9C%E4%B8%AD%E7%9A%84%E7%BD%91%E7%BB%9C"><span class="toc-text">NiN 网络中的网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#GoogLeNet"><span class="toc-text">GoogLeNet</span></a></li></ol>
  </aside>

        
      </div>
    </div>
  </div>
</main>
  

<footer class="footer">
  <div class="footer-social"><a 
        href="tencent://message/?Menu=yes&uin=1134384717 "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#12B7F5'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconQQ "></i>
      </a><a 
        href="javascript:; "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#09BB07'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconwechat-fill "></i>
      </a><a 
        href="https://www.instagram.com/ "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#DA2E76'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconinstagram "></i>
      </a><a 
        href="https://github.com/Squirrel00011 "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#9f7be1'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  icongithub-fill "></i>
      </a><a 
        href="mailto:1134384717@qq.com "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color=#ff3b00" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconmail"></i>
      </a></div>
  
    <div class="footer-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Theme - <a target="_blank" href="https://github.com/izhaoo/hexo-theme-zhaoo">zhaoo</a></p></div>
  
</footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  
    <div class="fab fab-like">
      <i class="iconfont iconheart"></i>
    </div>
  
  
    <div class="fab fab-daovoice">
      <i class="iconfont iconcomment"></i>
    </div>
  
  
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  
  
  
    
<script src="/js/color-mode.js"></script>

  
  
    <div class="search">
  <div class="search-container">
    <div class="search-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <div class="search-input-wrapper">
      <i class="search-input-icon iconfont iconsearch"></i>
      <input class="search-input" type="search" id="search-input" placeholder="Search..." autofocus autocomplete="off"
        autocorrect="off" autocapitalize="off">
    </div>
    <div class="search-output" id="search-output"></div>
  </div>
</div>
  
</body>

<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>





  
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>




  
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>






  
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.qrcode/1.0/jquery.qrcode.min.js"></script>




<script src="/js/utils.js"></script>
<script src="/js/script.js"></script>



  <script>
  $.getScript("//cdn.jsdelivr.net/npm/leancloud-storage@4.1.0/dist/av-min.js", () => {

    AV.init({
      appId: 'eSrfpdL3XjSFRaFUwnuRl3dn-gzGzoHsz',
      appKey: '6DvkgciWlfleRrT7w9r6snJI',
      serverURLs: 'https://leancloud.cn/',
    });

    const Counter = AV.Object.extend("Counter");
    const Like = AV.Object.extend("Like");

    const showCount = (Counter) => {
      const asyncLimit = new AsyncLimit(2);
      $(".leancloud-counter").each(async (e) => {
        const url = $(".leancloud-counter").eq(e).attr('id').trim();
        const query = new AV.Query("Counter");
        query.equalTo("words", url);
        let count = await asyncLimit.run(() => query.count());
        $(".leancloud-counter").eq(e).text(count ? count : 0);
      });
    }

    const addCount = (Counter) => {
      const url = $(".leancloud-counter").length === 1 ? $(".leancloud-counter").attr('id').trim() : 'https://hxiangdou.github.io';
      var query = new Counter;
      query.save({
        words: url
      });
    }

    const showLike = (Like) => {
      const asyncLimit = new AsyncLimit(2);
      $(".leancloud-like").each(async (e) => {
        const url = $(".leancloud-like").eq(e).attr('id').trim();
        const query = new AV.Query("Like");
        query.equalTo("path", url);
        let count = await asyncLimit.run(() => query.count());
        $(".leancloud-like").eq(e).text(count ? count : 0);
      });
    }

    const addLike = (Like) => {
      const url = $(".leancloud-like").length === 1 ? $(".leancloud-like").attr('id').trim() : 'https://hxiangdou.github.io';
      var query = new Like;
      query.save({
        path: url,
        nickName: 'Anonymous'
      });
      $(".leancloud-like").addClass('islike');
      $(".fab-like").children(".iconfont").removeClass("iconheart").addClass("iconheart-fill").css("color", "#eb3223");
      ZHAOO.zui.message({ text: '爱你哦~', type: 'success' });
      setTimeout(() => showLike(Like), 1000);
    }

    const handleLikeClick = () => {
      const isLike = $(".leancloud-like").length === 1 && $(".leancloud-like").hasClass('islike') ? true : false;
      if (isLike) {
        ZHAOO.zui.message({ text: '小心心不可以收回呢~', type: 'warning' });
      } else {
        addLike(Like);
      }
    }

    $(function () {
      addCount(Counter);
      showCount(Counter);
      showLike(Like);
      $(".fab-like").on("click", function () {
        handleLikeClick();
      });
    });

  });
</script>



  

<script>
  (function (i, s, o, g, r, a, m) {
    i["DaoVoiceObject"] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date();
    a = s.createElement(o), m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    a.charset = "utf-8";
    m.parentNode.insertBefore(a, m)
  })(window, document, "script", "https://widget.daovoice.io/widget/0f81ff2f.js", "daovoice")
  daovoice('init', {
    app_id: "38fcd752"
  }, {
    launcher: {
      disableLauncherIcon: true,
    },
  });
  daovoice('update');
</script>



  <script>
    (function () {
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>













</html>