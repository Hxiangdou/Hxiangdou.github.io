

<!DOCTYPE html>
<html lang="en" color-mode=light>
<head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <title>动手学习深度学习（13）现代卷积神经网络（2） - Hexo</title>
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="google" content="notranslate" />
  
  <meta name="description" content="训练深层神经网络是十分困难的，特别是在较短的时间内使他...">
  <meta name="author" content="茴香豆">
  <link rel="icon" href="/images/icons/favicon-16x16.png" type="image/png" sizes="16x16">
  <link rel="icon" href="/images/icons/favicon-32x32.png" type="image/png" sizes="32x32">
  <link rel="apple-touch-icon" href="/images/icons/apple-touch-icon.png" sizes="180x180">
  <meta rel="mask-icon" href="/images/icons/stun-logo.svg" color="#333333">
  
    <meta rel="msapplication-TileImage" content="/images/icons/favicon-144x144.png">
    <meta rel="msapplication-TileColor" content="#000000">
  

  
<link rel="stylesheet" href="/css/style.css">


  
    
<link rel="stylesheet" href="https://at.alicdn.com/t/font_1445822_p6ry5n7lrr.css">

  

  
    
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.css">

  

  
    
      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/xcode.min.css" name="highlight-style" mode="light">

      
        
        
<link rel="stylesheet" href="https://cdn.bootcss.com/highlight.js/9.18.1/styles/solarized-dark.min.css" name="highlight-style" mode="dark">

      
  

  <script>
    var CONFIG = window.CONFIG || {};
    var ZHAOO = window.ZHAOO || {};
    CONFIG = {
      isHome: false,
      fancybox: true,
      pjax: false,
      loading: {
        gif: '/images/theme/loading.gif',
        lottie: ''
      },
      lazyload: {
        enable: true,
        only_post: 'false',
        loading: {
          gif: '/images/theme/loading.gif',
          lottie: ''
        }
      },
      donate: {
        enable: true,
        alipay: '/images/支付宝.JPG',
        wechat: '/images/微信.JPG'
      },
      galleries: {
        enable: true
      },
      fab: {
        enable: true,
        always_show: false
      },
      carrier: {
        enable: true
      },
      daovoice: {
        enable: true
      },
      preview: {
        background: {
          default: '',
          api: ''
        },
        motto: {
          default: '我在开了灯的床头下，想问问自己的心啊。',
          typing: true,
          api: 'https://v2.jinrishici.com/one.json',
          data_contents: '["data","content"]'
        },
      },
      qrcode: {
        enable: true,
        type: 'url',
        image: 'https://pic.izhaoo.com/weapp-code.jpg',
      },
      toc: {
        enable: true
      },
      scrollbar: {
        type: 'default'
      },
      notification: {
        enable: false,
        delay: 4500,
        list: '',
        page_white_list: '',
        page_black_list: ''
      },
      search: {
        enable: true,
        path: '/search.xml'
      }
    }
  </script>

  

  

<!-- hexo injector head_end start -->
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css">

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/hexo-math@4.0.0/dist/style.css">
<!-- hexo injector head_end end --><meta name="generator" content="Hexo 6.2.0"></head>

<body class="lock-screen">
  <div class="loading" id="loading"></div>
  
    


  <nav class="navbar">
    <div class="left">
      
        <i class="iconfont iconhome j-navbar-back-home"></i>
      
      
        <i class="iconfont iconqrcode j-navbar-qrcode"></i>
      
      
        <i class="iconfont iconmoono" id="color-toggle" color-toggle="light"></i>
      
      
        <i class="iconfont iconsearch j-navbar-search"></i>
      
    </div>
    <div class="center">动手学习深度学习（13）现代卷积神经网络（2）</div>
    <div class="right">
      <i class="iconfont iconmenu j-navbar-menu"></i>
    </div>
    
      <div id="qrcode-navbar"></div>
    
  </nav>

  
  

<nav class="menu">
  <div class="menu-container">
    <div class="menu-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <ul class="menu-content"><li class="menu-item">
        <a href="/ " class="underline "> 首页</a>
      </li><li class="menu-item">
        <a href="/galleries/ " class="underline "> 相册</a>
      </li><li class="menu-item">
        <a href="/archives/ " class="underline "> 归档</a>
      </li><li class="menu-item">
        <a href="/tags/ " class="underline "> 标签</a>
      </li><li class="menu-item">
        <a href="/categories/ " class="underline "> 分类</a>
      </li><li class="menu-item">
        <a href="/about/ " class="underline "> 关于</a>
      </li></ul>
    
      <div class="menu-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Theme - <a target="_blank" href="https://github.com/izhaoo/hexo-theme-zhaoo">zhaoo</a></p></div>
    
  </div>
</nav>
  <main id="main">
  <div class="article-wrap">
    <div class="row container">
      <div class="col-xl-3"></div>
      <div class="col-xl-6"><article class="article">
  <div class="wrap">
    <section class="head">
  <img   class="lazyload" data-original="/images/pat/2.jpg" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg=="  draggable="false">
  <div class="head-mask">
    <h1 class="head-title">动手学习深度学习（13）现代卷积神经网络（2）</h1>
    <div class="head-info">
      <span class="post-info-item"><i class="iconfont iconcalendar"></i>November 01, 2022</span>
      
        <span class="post-info-item">
          <i class="iconfont iconeye"></i><span id="/2022/11/01/DL_13/" class="leancloud-counter" data-flag-title="动手学习深度学习（13）现代卷积神经网络（2）"></span>
        </span>
        <span class="post-info-item">
          <i class="iconfont iconheart"></i><span id="/2022/11/01/DL_13/" class="leancloud-like" data-flag-title="动手学习深度学习（13）现代卷积神经网络（2）"></span>
        </span>
      
      <span class="post-info-item"><i class="iconfont iconfont-size"></i>7368</span>
    </div>
  </div>
</section>
    <section class="main">
      <section class="content">
        
        <p>训练深层神经网络是十分困难的，特别是在较短的时间内使他们收敛更加棘手。 在本节中，我们将介绍<em>批量规范化</em>（batch normalization），这是一种流行且有效的技术，可持续加速深层网络的收敛速度。</p>
<h2 id="批量归一化"><a href="#批量归一化" class="headerlink" title="批量归一化"></a>批量归一化</h2><p>固定小批量里面的均值和方差，批量归一化是线性变换<br>MATHJAX-SSR-6</p>
<p style="text-align:center"><svg xmlns:xlink="http://www.w3.org/1999/xlink" width="27.06ex" height="6.176ex" style="vertical-align: -2.338ex;" viewBox="0 -1652.5 11650.8 2659.1" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">\mathrm{BN}(\mathbf{x}) = \boldsymbol{\gamma} \odot \frac{\mathbf{x} - \hat{\boldsymbol{\mu}}_\mathcal{B}}{\hat{\boldsymbol{\sigma}}_\mathcal{B}} + \boldsymbol{\beta}.</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMAIN-42" d="M131 622Q124 629 120 631T104 634T61 637H28V683H229H267H346Q423 683 459 678T531 651Q574 627 599 590T624 512Q624 461 583 419T476 360L466 357Q539 348 595 302T651 187Q651 119 600 67T469 3Q456 1 242 0H28V46H61Q103 47 112 49T131 61V622ZM511 513Q511 560 485 594T416 636Q415 636 403 636T371 636T333 637Q266 637 251 636T232 628Q229 624 229 499V374H312L396 375L406 377Q410 378 417 380T442 393T474 417T499 456T511 513ZM537 188Q537 239 509 282T430 336L329 337H229V200V116Q229 57 234 52Q240 47 334 47H383Q425 47 443 53Q486 67 511 104T537 188Z"></path>
<path stroke-width="1" id="E1-MJMAIN-4E" d="M42 46Q74 48 94 56T118 69T128 86V634H124Q114 637 52 637H25V683H232L235 680Q237 679 322 554T493 303L578 178V598Q572 608 568 613T544 627T492 637H475V683H483Q498 680 600 680Q706 680 715 683H724V637H707Q634 633 622 598L621 302V6L614 0H600Q585 0 582 3T481 150T282 443T171 605V345L172 86Q183 50 257 46H274V0H265Q250 3 150 3Q48 3 33 0H25V46H42Z"></path>
<path stroke-width="1" id="E1-MJMAIN-28" d="M94 250Q94 319 104 381T127 488T164 576T202 643T244 695T277 729T302 750H315H319Q333 750 333 741Q333 738 316 720T275 667T226 581T184 443T167 250T184 58T225 -81T274 -167T316 -220T333 -241Q333 -250 318 -250H315H302L274 -226Q180 -141 137 -14T94 250Z"></path>
<path stroke-width="1" id="E1-MJMAINB-78" d="M227 0Q212 3 121 3Q40 3 28 0H21V62H117L245 213L109 382H26V444H34Q49 441 143 441Q247 441 265 444H274V382H246L281 339Q315 297 316 297Q320 297 354 341L389 382H352V444H360Q375 441 466 441Q547 441 559 444H566V382H471L355 246L504 63L545 62H586V0H578Q563 3 469 3Q365 3 347 0H338V62H366Q366 63 326 112T285 163L198 63L217 62H235V0H227Z"></path>
<path stroke-width="1" id="E1-MJMAIN-29" d="M60 749L64 750Q69 750 74 750H86L114 726Q208 641 251 514T294 250Q294 182 284 119T261 12T224 -76T186 -143T145 -194T113 -227T90 -246Q87 -249 86 -250H74Q66 -250 63 -250T58 -247T55 -238Q56 -237 66 -225Q221 -64 221 250T66 725Q56 737 55 738Q55 746 60 749Z"></path>
<path stroke-width="1" id="E1-MJMAIN-3D" d="M56 347Q56 360 70 367H707Q722 359 722 347Q722 336 708 328L390 327H72Q56 332 56 347ZM56 153Q56 168 72 173H708Q722 163 722 153Q722 140 707 133H70Q56 140 56 153Z"></path>
<path stroke-width="1" id="E1-MJMATHBI-3B3" d="M5 269Q5 285 19 312T57 368T124 421T215 451H241Q274 451 303 439T353 406T389 361T416 311T432 266T442 232L444 220L446 216L450 226Q473 278 513 357T561 441Q566 444 584 444H594Q617 444 617 430Q617 426 596 389T536 273T462 110L452 84L451 70Q447 12 427 -76T388 -192Q375 -211 355 -211Q339 -211 332 -198T325 -171Q325 -114 386 64L393 84V98Q393 181 371 241Q360 280 319 303T210 327Q158 327 126 317T84 296T68 272T59 258Q55 256 36 256Q23 256 18 256T9 260T5 269Z"></path>
<path stroke-width="1" id="E1-MJMAIN-2299" d="M56 250Q56 394 156 488T384 583Q530 583 626 485T722 250Q722 110 625 14T390 -83Q249 -83 153 14T56 250ZM682 250Q682 322 649 387T546 497T381 542Q272 542 184 459T95 250Q95 132 178 45T389 -42Q515 -42 598 45T682 250ZM311 250Q311 285 332 304T375 328Q376 328 382 328T392 329Q424 326 445 305T466 250Q466 217 445 195T389 172Q354 172 333 195T311 250Z"></path>
<path stroke-width="1" id="E1-MJMAIN-2212" d="M84 237T84 250T98 270H679Q694 262 694 250T679 230H98Q84 237 84 250Z"></path>
<path stroke-width="1" id="E1-MJMATHBI-3BC" d="M294 -8Q265 -8 244 -5T213 1T201 4Q200 4 192 -32T172 -111T155 -168Q134 -211 86 -211Q62 -211 48 -196T34 -158Q37 -144 103 123T174 404Q182 424 201 438T244 452Q271 452 284 436T298 404Q298 392 267 269T235 114Q235 43 305 43Q342 43 375 68T418 110Q420 112 455 253T492 397Q514 444 562 444Q587 444 601 429T615 397Q615 387 599 320T563 178T542 93Q540 81 540 72Q540 42 558 42Q580 42 596 75Q606 94 616 134Q621 155 624 158T646 162H651H662Q682 162 682 148Q681 142 679 132T665 94T641 47T602 9T548 -8Q523 -8 502 -3T468 11T446 27T432 40L429 46Q367 -8 294 -8Z"></path>
<path stroke-width="1" id="E1-MJMAIN-5E" d="M112 560L249 694L257 686Q387 562 387 560L361 531Q359 532 303 581L250 627L195 580Q182 569 169 557T148 538L140 532Q138 530 125 546L112 560Z"></path>
<path stroke-width="1" id="E1-MJCAL-42" d="M304 342Q292 342 292 353Q292 372 323 391Q331 396 417 428T533 487Q563 512 563 555V562Q563 575 557 589T530 618T475 636Q429 636 396 613T330 539Q263 446 210 238Q196 183 173 120Q135 31 121 16Q108 1 85 -10T47 -22T32 -10Q32 -5 44 18T77 93T112 206Q135 296 154 395T182 550T191 615Q191 616 190 616Q188 616 179 611T157 601T131 594Q113 594 113 605Q113 623 144 644Q154 650 205 676T267 703Q277 705 279 705Q295 705 295 693Q295 686 288 635T278 575Q278 572 287 582Q336 635 402 669T540 704Q603 704 633 673T664 599Q664 559 638 523T580 462Q553 440 504 413L491 407L504 402Q566 381 596 338T627 244Q627 172 575 110T444 13T284 -22Q208 -22 158 28Q144 42 146 50Q150 67 178 85T230 103Q236 103 246 95T267 75T302 56T357 47Q436 47 486 93Q526 136 526 198V210Q526 228 518 249T491 292T436 330T350 345Q335 345 321 344T304 342Z"></path>
<path stroke-width="1" id="E1-MJMATHBI-3C3" d="M35 151Q35 190 51 236T99 327T184 404T306 443Q307 443 316 443T342 443T378 444T425 444T476 444Q606 444 626 444T655 439Q677 426 677 400Q677 358 639 340Q625 333 563 333Q510 333 510 331Q518 319 518 272Q518 155 437 74T226 -8Q123 -8 79 41T35 151ZM396 278Q396 314 375 323T305 332Q249 332 222 310T180 243Q171 219 162 178T153 116V110Q153 43 234 43Q347 43 382 199Q383 203 383 204Q396 255 396 278Z"></path>
<path stroke-width="1" id="E1-MJMAIN-2B" d="M56 237T56 250T70 270H369V420L370 570Q380 583 389 583Q402 583 409 568V270H707Q722 262 722 250T707 230H409V-68Q401 -82 391 -82H389H387Q375 -82 369 -68V230H70Q56 237 56 250Z"></path>
<path stroke-width="1" id="E1-MJMATHBI-3B2" d="M59 -194H49Q31 -194 28 -182Q28 -178 107 139T192 473Q212 533 248 580T324 652T395 689T450 701H461Q514 701 551 688T605 652T630 607T637 561Q637 546 634 526T611 465T556 393Q572 382 590 347T608 262Q608 146 522 69T299 -8Q279 -8 261 -6T228 2T204 13T183 26T169 37T157 48L150 56L120 -64Q113 -90 104 -128Q93 -175 89 -184T73 -194H59ZM531 592Q531 651 463 651Q399 651 341 600T253 466Q250 458 217 327T182 185Q180 176 180 159Q180 108 212 76T301 44Q330 44 354 51T393 65T423 91T444 118T459 151T468 179T475 206Q490 264 491 296Q491 313 489 326T484 345L482 350Q481 350 477 348T464 344T444 340T413 335T372 333T334 334T301 340T274 355T265 380Q265 444 397 444Q425 444 445 441T476 436L485 433Q489 433 499 458Q509 482 520 527T531 592ZM424 390Q424 393 389 393Q383 393 374 393T362 392Q348 392 333 388Q345 384 379 384Q424 384 424 390Z"></path>
<path stroke-width="1" id="E1-MJMAIN-2E" d="M78 60Q78 84 95 102T138 120Q162 120 180 104T199 61Q199 36 182 18T139 0T96 17T78 60Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMAIN-42" x="0" y="0"></use>
 <use xlink:href="#E1-MJMAIN-4E" x="708" y="0"></use>
 <use xlink:href="#E1-MJMAIN-28" x="1459" y="0"></use>
 <use xlink:href="#E1-MJMAINB-78" x="1848" y="0"></use>
 <use xlink:href="#E1-MJMAIN-29" x="2456" y="0"></use>
 <use xlink:href="#E1-MJMAIN-3D" x="3123" y="0"></use>
 <use xlink:href="#E1-MJMATHBI-3B3" x="4179" y="0"></use>
 <use xlink:href="#E1-MJMAIN-2299" x="5019" y="0"></use>
<g transform="translate(5797,0)">
<g transform="translate(342,0)">
<rect stroke="none" width="3228" height="60" x="0" y="220"></rect>
<g transform="translate(60,783)">
 <use xlink:href="#E1-MJMAINB-78" x="0" y="0"></use>
 <use xlink:href="#E1-MJMAIN-2212" x="829" y="0"></use>
<g transform="translate(1830,0)">
 <use xlink:href="#E1-MJMATHBI-3BC" x="0" y="0"></use>
 <use xlink:href="#E1-MJMAIN-5E" x="135" y="30"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJCAL-42" x="1001" y="-350"></use>
</g>
</g>
<g transform="translate(986,-739)">
 <use xlink:href="#E1-MJMATHBI-3C3" x="0" y="0"></use>
 <use xlink:href="#E1-MJMAIN-5E" x="93" y="23"></use>
 <use transform="scale(0.707)" xlink:href="#E1-MJCAL-42" x="970" y="-219"></use>
</g>
</g>
</g>
 <use xlink:href="#E1-MJMAIN-2B" x="9711" y="0"></use>
 <use xlink:href="#E1-MJMATHBI-3B2" x="10711" y="0"></use>
 <use xlink:href="#E1-MJMAIN-2E" x="11372" y="0"></use>
</g>
</svg></p>
<p>均值 <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.332ex" height="2.509ex" style="vertical-align: -0.671ex;" viewBox="0 -791.3 573.5 1080.4" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">\beta</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMATHI-3B2" d="M29 -194Q23 -188 23 -186Q23 -183 102 134T186 465Q208 533 243 584T309 658Q365 705 429 705H431Q493 705 533 667T573 570Q573 465 469 396L482 383Q533 332 533 252Q533 139 448 65T257 -10Q227 -10 203 -2T165 17T143 40T131 59T126 65L62 -188Q60 -194 42 -194H29ZM353 431Q392 431 427 419L432 422Q436 426 439 429T449 439T461 453T472 471T484 495T493 524T501 560Q503 569 503 593Q503 611 502 616Q487 667 426 667Q384 667 347 643T286 582T247 514T224 455Q219 439 186 308T152 168Q151 163 151 147Q151 99 173 68Q204 26 260 26Q302 26 349 51T425 137Q441 171 449 214T457 279Q457 337 422 372Q380 358 347 358H337Q258 358 258 389Q258 396 261 403Q275 431 353 431Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMATHI-3B2" x="0" y="0"></use>
</g>
</svg> 和方差 <svg xmlns:xlink="http://www.w3.org/1999/xlink" width="1.262ex" height="2.176ex" style="vertical-align: -0.838ex;" viewBox="0 -576.1 543.5 936.9" role="img" focusable="false" xmlns="http://www.w3.org/2000/svg" aria-labelledby="MathJax-SVG-1-Title">
<title id="MathJax-SVG-1-Title">\gamma</title>
<defs aria-hidden="true">
<path stroke-width="1" id="E1-MJMATHI-3B3" d="M31 249Q11 249 11 258Q11 275 26 304T66 365T129 418T206 441Q233 441 239 440Q287 429 318 386T371 255Q385 195 385 170Q385 166 386 166L398 193Q418 244 443 300T486 391T508 430Q510 431 524 431H537Q543 425 543 422Q543 418 522 378T463 251T391 71Q385 55 378 6T357 -100Q341 -165 330 -190T303 -216Q286 -216 286 -188Q286 -138 340 32L346 51L347 69Q348 79 348 100Q348 257 291 317Q251 355 196 355Q148 355 108 329T51 260Q49 251 47 251Q45 249 31 249Z"></path>
</defs>
<g stroke="currentColor" fill="currentColor" stroke-width="0" transform="matrix(1 0 0 -1 0 0)" aria-hidden="true">
 <use xlink:href="#E1-MJMATHI-3B3" x="0" y="0"></use>
</g>
</svg> 为学习参数。</p>
<p>作用在</p>
<ul>
<li>全连接层和卷积层输出上，激活函数前</li>
<li>全连接层和卷积层输入上</li>
</ul>
<p>全连接层，作用在特征维；卷积层，作用在通道维。</p>
<p>可以加速收敛速度（通过匀速更大的学习率），但一般不改变模型精度。</p>
<h3 id="从零实现"><a href="#从零实现" class="headerlink" title="从零实现"></a>从零实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">batch_norm</span>(<span class="hljs-params">X, gamma, beta, moving_mean, moving_var, eps, momentum</span>):<br>    <span class="hljs-comment"># 通过is_grad_enabled来判断当前模式是训练模式还是预测模式</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> torch.is_grad_enabled():<br>        <span class="hljs-comment"># 如果是在预测模式下，直接使用传入的移动平均所得的均值和方差</span><br>        X_hat = (X - moving_mean) / torch.sqrt(moving_var + eps)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-keyword">assert</span> <span class="hljs-built_in">len</span>(X.shape) <span class="hljs-keyword">in</span> (<span class="hljs-number">2</span>, <span class="hljs-number">4</span>)<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">len</span>(X.shape) == <span class="hljs-number">2</span>:<br>            <span class="hljs-comment"># 使用全连接层的情况，计算特征维上的均值和方差</span><br>            mean = X.mean(dim=<span class="hljs-number">0</span>)<br>            var = ((X - mean) ** <span class="hljs-number">2</span>).mean(dim=<span class="hljs-number">0</span>)<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># 使用二维卷积层的情况，计算通道维上（axis=1）的均值和方差。</span><br>            <span class="hljs-comment"># 这里我们需要保持X的形状以便后面可以做广播运算</span><br>            mean = X.mean(dim=(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), keepdim=<span class="hljs-literal">True</span>)<br>            var = ((X - mean) ** <span class="hljs-number">2</span>).mean(dim=(<span class="hljs-number">0</span>, <span class="hljs-number">2</span>, <span class="hljs-number">3</span>), keepdim=<span class="hljs-literal">True</span>)<br>        <span class="hljs-comment"># 训练模式下，用当前的均值和方差做标准化</span><br>        X_hat = (X - mean) / torch.sqrt(var + eps)<br>        <span class="hljs-comment"># 更新移动平均的均值和方差</span><br>        moving_mean = momentum * moving_mean + (<span class="hljs-number">1.0</span> - momentum) * mean<br>        moving_var = momentum * moving_var + (<span class="hljs-number">1.0</span> - momentum) * var<br>    Y = gamma * X_hat + beta  <span class="hljs-comment"># 缩放和移位</span><br>    <span class="hljs-keyword">return</span> Y, moving_mean.data, moving_var.data<br></code></pre></td></tr></table></figure>

<p>创建一个正确的<code>BatchNorm</code>图层</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">class</span> <span class="hljs-title class_">BatchNorm</span>(nn.Module):<br>    <span class="hljs-comment"># num_features：完全连接层的输出数量或卷积层的输出通道数。</span><br>    <span class="hljs-comment"># num_dims：2表示完全连接层，4表示卷积层</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, num_features, num_dims</span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        <span class="hljs-keyword">if</span> num_dims == <span class="hljs-number">2</span>:<br>            shape = (<span class="hljs-number">1</span>, num_features)<br>        <span class="hljs-keyword">else</span>:<br>            shape = (<span class="hljs-number">1</span>, num_features, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>)<br>        <span class="hljs-comment"># 参与求梯度和迭代的拉伸和偏移参数，分别初始化成1和0</span><br>        self.gamma = nn.Parameter(torch.ones(shape))<br>        self.beta = nn.Parameter(torch.zeros(shape))<br>        <span class="hljs-comment"># 非模型参数的变量初始化为0和1</span><br>        self.moving_mean = torch.zeros(shape)<br>        self.moving_var = torch.ones(shape)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        <span class="hljs-comment"># 如果X不在内存上，将moving_mean和moving_var</span><br>        <span class="hljs-comment"># 复制到X所在显存上</span><br>        <span class="hljs-keyword">if</span> self.moving_mean.device != X.device:<br>            self.moving_mean = self.moving_mean.to(X.device)<br>            self.moving_var = self.moving_var.to(X.device)<br>        <span class="hljs-comment"># 保存更新过的moving_mean和moving_var</span><br>        Y, self.moving_mean, self.moving_var = batch_norm(<br>            X, self.gamma, self.beta, self.moving_mean,<br>            self.moving_var, eps=<span class="hljs-number">1e-5</span>, momentum=<span class="hljs-number">0.9</span>)<br>        <span class="hljs-keyword">return</span> Y<br></code></pre></td></tr></table></figure>

<p>应用<code>BatchNorm</code>于LeNet模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python">net = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>), BatchNorm(<span class="hljs-number">6</span>, num_dims=<span class="hljs-number">4</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>), BatchNorm(<span class="hljs-number">16</span>, num_dims=<span class="hljs-number">4</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>), nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">16</span>*<span class="hljs-number">4</span>*<span class="hljs-number">4</span>, <span class="hljs-number">120</span>), BatchNorm(<span class="hljs-number">120</span>, num_dims=<span class="hljs-number">2</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>), BatchNorm(<span class="hljs-number">84</span>, num_dims=<span class="hljs-number">2</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure>

<p>在Fashion-MNIST数据集上的效果</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">1.0</span>, <span class="hljs-number">10</span>, <span class="hljs-number">256</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br><span class="hljs-comment"># output</span><br>loss <span class="hljs-number">0.268</span>, train acc <span class="hljs-number">0.900</span>, test acc <span class="hljs-number">0.831</span><br><span class="hljs-number">38739.6</span> examples/sec on cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>

<h3 id="简洁实现"><a href="#简洁实现" class="headerlink" title="简洁实现"></a>简洁实现</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><code class="hljs python">net = nn.Sequential(<br>    nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">6</span>, kernel_size=<span class="hljs-number">5</span>), nn.BatchNorm2d(<span class="hljs-number">6</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>),<br>    nn.Conv2d(<span class="hljs-number">6</span>, <span class="hljs-number">16</span>, kernel_size=<span class="hljs-number">5</span>), nn.BatchNorm2d(<span class="hljs-number">16</span>), nn.Sigmoid(),<br>    nn.AvgPool2d(kernel_size=<span class="hljs-number">2</span>, stride=<span class="hljs-number">2</span>), nn.Flatten(),<br>    nn.Linear(<span class="hljs-number">256</span>, <span class="hljs-number">120</span>), nn.BatchNorm1d(<span class="hljs-number">120</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">120</span>, <span class="hljs-number">84</span>), nn.BatchNorm1d(<span class="hljs-number">84</span>), nn.Sigmoid(),<br>    nn.Linear(<span class="hljs-number">84</span>, <span class="hljs-number">10</span>))<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br><span class="hljs-comment"># output</span><br>loss <span class="hljs-number">0.269</span>, train acc <span class="hljs-number">0.901</span>, test acc <span class="hljs-number">0.853</span><br><span class="hljs-number">64557.2</span> examples/sec on cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>

<h2 id="ResNet残差网络"><a href="#ResNet残差网络" class="headerlink" title="ResNet残差网络"></a>ResNet残差网络</h2><p><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1bV41177ap/?spm_id_from=333.999.0.0&vd_source=f1e7eb1d150afc7b732a2b8c557e6d35">残差网络 ResNet【动手学深度学习v2】</a></p>
<p>通过视频可以更直观的理解残差网络的作用和改进之处。</p>
<p>残差块的实现</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> torch <span class="hljs-keyword">import</span> nn<br><span class="hljs-keyword">from</span> torch.nn <span class="hljs-keyword">import</span> functional <span class="hljs-keyword">as</span> F<br><span class="hljs-keyword">from</span> d2l <span class="hljs-keyword">import</span> torch <span class="hljs-keyword">as</span> d2l<br><br><br><span class="hljs-keyword">class</span> <span class="hljs-title class_">Residual</span>(nn.Module):  <span class="hljs-comment">#@save</span><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">__init__</span>(<span class="hljs-params">self, input_channels, num_channels,</span><br><span class="hljs-params">                 use_1x1conv=<span class="hljs-literal">False</span>, strides=<span class="hljs-number">1</span></span>):<br>        <span class="hljs-built_in">super</span>().__init__()<br>        self.conv1 = nn.Conv2d(input_channels, num_channels,<br>                               kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>, stride=strides)<br>        self.conv2 = nn.Conv2d(num_channels, num_channels,<br>                               kernel_size=<span class="hljs-number">3</span>, padding=<span class="hljs-number">1</span>)<br>        <span class="hljs-keyword">if</span> use_1x1conv:<br>            self.conv3 = nn.Conv2d(input_channels, num_channels,<br>                                   kernel_size=<span class="hljs-number">1</span>, stride=strides)<br>        <span class="hljs-keyword">else</span>:<br>            self.conv3 = <span class="hljs-literal">None</span><br>        self.bn1 = nn.BatchNorm2d(num_channels)<br>        self.bn2 = nn.BatchNorm2d(num_channels)<br><br>    <span class="hljs-keyword">def</span> <span class="hljs-title function_">forward</span>(<span class="hljs-params">self, X</span>):<br>        Y = F.relu(self.bn1(self.conv1(X)))<br>        Y = self.bn2(self.conv2(Y))<br>        <span class="hljs-keyword">if</span> self.conv3:<br>            X = self.conv3(X)<br>        Y += X<br>        <span class="hljs-keyword">return</span> F.relu(Y)<br></code></pre></td></tr></table></figure>

<p>查看输入和输出形状一致的情况</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">blk = Residual(<span class="hljs-number">3</span>,<span class="hljs-number">3</span>)<br>X = torch.rand(<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>)<br>Y = blk(X)<br><span class="hljs-built_in">print</span>(Y.shape)<br><span class="hljs-comment"># output</span><br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">3</span>, <span class="hljs-number">6</span>, <span class="hljs-number">6</span>])<br></code></pre></td></tr></table></figure>

<p>我们也可以在增加输出通道数的同时，减半输出的高和宽。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">blk = Residual(<span class="hljs-number">3</span>,<span class="hljs-number">6</span>, use_1x1conv=<span class="hljs-literal">True</span>, strides=<span class="hljs-number">2</span>)<br><span class="hljs-built_in">print</span>(blk(X).shape)<br><span class="hljs-comment"># output</span><br>torch.Size([<span class="hljs-number">4</span>, <span class="hljs-number">6</span>, <span class="hljs-number">3</span>, <span class="hljs-number">3</span>])<br></code></pre></td></tr></table></figure>

<h3 id="ResNet模型"><a href="#ResNet模型" class="headerlink" title="ResNet模型"></a>ResNet模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><code class="hljs python">b1 = nn.Sequential(nn.Conv2d(<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, kernel_size=<span class="hljs-number">7</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">3</span>),<br>                   nn.BatchNorm2d(<span class="hljs-number">64</span>), nn.ReLU(),<br>                   nn.MaxPool2d(kernel_size=<span class="hljs-number">3</span>, stride=<span class="hljs-number">2</span>, padding=<span class="hljs-number">1</span>))<br><span class="hljs-comment"># 注意我们对第一个模块做了特别处理</span><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">resnet_block</span>(<span class="hljs-params">input_channels, num_channels, num_residuals,</span><br><span class="hljs-params">                 first_block=<span class="hljs-literal">False</span></span>):<br>    blk = []<br>    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(num_residuals):<br>        <span class="hljs-keyword">if</span> i == <span class="hljs-number">0</span> <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> first_block:<br>            blk.append(Residual(input_channels, num_channels,<br>                                use_1x1conv=<span class="hljs-literal">True</span>, strides=<span class="hljs-number">2</span>))<br>        <span class="hljs-keyword">else</span>:<br>            blk.append(Residual(num_channels, num_channels))<br>    <span class="hljs-keyword">return</span> blk<br><span class="hljs-comment"># 接着在ResNet加入所有残差块，每个模块使用2个残差块</span><br>b2 = nn.Sequential(*resnet_block(<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">2</span>, first_block=<span class="hljs-literal">True</span>))<br>b3 = nn.Sequential(*resnet_block(<span class="hljs-number">64</span>, <span class="hljs-number">128</span>, <span class="hljs-number">2</span>))<br>b4 = nn.Sequential(*resnet_block(<span class="hljs-number">128</span>, <span class="hljs-number">256</span>, <span class="hljs-number">2</span>))<br>b5 = nn.Sequential(*resnet_block(<span class="hljs-number">256</span>, <span class="hljs-number">512</span>, <span class="hljs-number">2</span>))<br><span class="hljs-comment"># 在ResNet中加入全剧平均汇聚层，以及全连接输出。</span><br>net = nn.Sequential(b1, b2, b3, b4, b5,<br>                    nn.AdaptiveAvgPool2d((<span class="hljs-number">1</span>,<span class="hljs-number">1</span>)),<br>                    nn.Flatten(), nn.Linear(<span class="hljs-number">512</span>, <span class="hljs-number">10</span>))<br></code></pre></td></tr></table></figure>

<p>在训练ResNet之前，让我们观察一下ResNet中不同模块的输入形状是如何变化的。 在之前所有架构中，分辨率降低，通道数量增加，直到全局平均汇聚层聚集所有特征。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><code class="hljs python">X = torch.rand(size=(<span class="hljs-number">1</span>, <span class="hljs-number">1</span>, <span class="hljs-number">224</span>, <span class="hljs-number">224</span>))<br><span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> net:<br>    X = layer(X)<br>    <span class="hljs-built_in">print</span>(layer.__class__.__name__,<span class="hljs-string">&#x27;output shape:\t&#x27;</span>, X.shape)<br><span class="hljs-comment"># output</span><br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">56</span>, <span class="hljs-number">56</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">64</span>, <span class="hljs-number">56</span>, <span class="hljs-number">56</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">128</span>, <span class="hljs-number">28</span>, <span class="hljs-number">28</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">256</span>, <span class="hljs-number">14</span>, <span class="hljs-number">14</span>])<br>Sequential output shape:     torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">7</span>, <span class="hljs-number">7</span>])<br>AdaptiveAvgPool2d output shape:      torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">512</span>, <span class="hljs-number">1</span>, <span class="hljs-number">1</span>])<br>Flatten output shape:        torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">512</span>])<br>Linear output shape:         torch.Size([<span class="hljs-number">1</span>, <span class="hljs-number">10</span>])<br></code></pre></td></tr></table></figure>

<p>训练模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs python">lr, num_epochs, batch_size = <span class="hljs-number">0.05</span>, <span class="hljs-number">10</span>, <span class="hljs-number">256</span><br>train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size, resize=<span class="hljs-number">96</span>)<br>d2l.train_ch6(net, train_iter, test_iter, num_epochs, lr, d2l.try_gpu())<br><span class="hljs-comment"># output</span><br>loss <span class="hljs-number">0.011</span>, train acc <span class="hljs-number">0.997</span>, test acc <span class="hljs-number">0.915</span><br><span class="hljs-number">4701.1</span> examples/sec on cuda:<span class="hljs-number">0</span><br></code></pre></td></tr></table></figure>

      </section>
      <section class="extra">
        
          <ul class="copyright">
  
    <li><strong>本文作者：</strong>茴香豆</li>
    <li><strong>本文链接：</strong><a href="https://hxiangdou.github.io/2022/11/01/DL_13/index.html" title="https:&#x2F;&#x2F;hxiangdou.github.io&#x2F;2022&#x2F;11&#x2F;01&#x2F;DL_13&#x2F;index.html">https:&#x2F;&#x2F;hxiangdou.github.io&#x2F;2022&#x2F;11&#x2F;01&#x2F;DL_13&#x2F;index.html</a></li>
    <li><strong>版权声明：</strong>本博客所有文章均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/deed.zh" title="BY-NC-SA" target="_blank" rel="noopener">BY-NC-SA</a> 许可协议，转载请注明出处！</li>
  
</ul>
        
        
          <section class="donate">
  <div id="qrcode-donate">
    <img   class="lazyload" data-original="/images/支付宝.JPG" src="data:image/gif;base64,iVBORw0KGgoAAAANSUhEUgAAAAEAAAABCAYAAAAfFcSJAAAADUlEQVQImWNgYGBgAAAABQABh6FO1AAAAABJRU5ErkJggg==" >
  </div>
  <div class="icon">
    <a href="javascript:;" id="alipay"><i class="iconfont iconalipay"></i></a>
    <a href="javascript:;" id="wechat"><i class="iconfont iconwechat-fill"></i></a>
  </div>
</section>
        
        
  <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/DeepLearning/" rel="tag">DeepLearning</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Python/" rel="tag">Python</a></li></ul> 

        
  <nav class="nav">
    <a href="/2022/11/04/Latex/"><i class="iconfont iconleft"></i>LaTex学习</a>
    <a href="/2022/10/31/DL_12/">动手学习深度学习（12）现代卷积神经网络（1）<i class="iconfont iconright"></i></a>
  </nav>

      </section>
      
        <section class="comments">
  
    <div class="btn" id="comments-btn">查看评论</div>
  
  
<div id="valine"></div>
<script defer src="//unpkg.com/valine/dist/Valine.min.js"></script>
<script>
  window.onload = function () {
    var loadValine = function () {
      new Valine({
        el: '#valine',
        app_id: "eSrfpdL3XjSFRaFUwnuRl3dn-gzGzoHsz",
        app_key: "6DvkgciWlfleRrT7w9r6snJI",
        placeholder: "雁过留痕",
        avatar: "mp",
        pageSize: "10",
        lang: "zh-CN",
      });
    }
    if ( true ) {
      $("#comments-btn").on("click", function () {
        $(this).hide();
        loadValine();
      });
    } else {
      loadValine();
    }
  };
</script>

</section>
      
    </section>
  </div>
</article></div>
      <div class="col-xl-3">
        
          
  <aside class="toc-wrap">
    <h3 class="toc-title">文章目录：</h3>
    <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#%E6%89%B9%E9%87%8F%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">批量归一化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%BB%8E%E9%9B%B6%E5%AE%9E%E7%8E%B0"><span class="toc-text">从零实现</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%AE%80%E6%B4%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">简洁实现</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ResNet%E6%AE%8B%E5%B7%AE%E7%BD%91%E7%BB%9C"><span class="toc-text">ResNet残差网络</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#ResNet%E6%A8%A1%E5%9E%8B"><span class="toc-text">ResNet模型</span></a></li></ol></li></ol>
  </aside>

        
      </div>
    </div>
  </div>
</main>
  

<footer class="footer">
  <div class="footer-social"><a 
        href="tencent://message/?Menu=yes&uin=1134384717 "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#12B7F5'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconQQ "></i>
      </a><a 
        href="javascript:; "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#09BB07'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconwechat-fill "></i>
      </a><a 
        href="https://www.instagram.com/ "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#DA2E76'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconinstagram "></i>
      </a><a 
        href="https://github.com/Squirrel00011 "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color= '#9f7be1'" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  icongithub-fill "></i>
      </a><a 
        href="mailto:1134384717@qq.com "
        target="_blank"
        class="footer-social-item"
        onMouseOver="this.style.color=#ff3b00" 
        onMouseOut="this.style.color='#33333D'">
          <i class="iconfont  iconmail"></i>
      </a></div>
  
    <div class="footer-copyright"><p>Powered by <a target="_blank" href="https://hexo.io">Hexo</a>  |  Theme - <a target="_blank" href="https://github.com/izhaoo/hexo-theme-zhaoo">zhaoo</a></p></div>
  
</footer>
  
      <div class="fab fab-plus">
    <i class="iconfont iconplus"></i>
  </div>
  
    <div class="fab fab-like">
      <i class="iconfont iconheart"></i>
    </div>
  
  
    <div class="fab fab-daovoice">
      <i class="iconfont iconcomment"></i>
    </div>
  
  
  <div class="fab fab-up">
    <i class="iconfont iconcaret-up"></i>
  </div>
  
  
  
    
<script src="/js/color-mode.js"></script>

  
  
    <div class="search">
  <div class="search-container">
    <div class="search-close">
      <i class="iconfont iconbaseline-close-px"></i>
    </div>
    <div class="search-input-wrapper">
      <i class="search-input-icon iconfont iconsearch"></i>
      <input class="search-input" type="search" id="search-input" placeholder="Search..." autofocus autocomplete="off"
        autocorrect="off" autocapitalize="off">
    </div>
    <div class="search-output" id="search-output"></div>
  </div>
</div>
  
</body>

<script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>





  
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.lazyload/1.9.1/jquery.lazyload.min.js"></script>




  
<script src="https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.5.7/jquery.fancybox.min.js"></script>






  
<script src="https://cdn.bootcdn.net/ajax/libs/jquery.qrcode/1.0/jquery.qrcode.min.js"></script>




<script src="/js/utils.js"></script>
<script src="/js/script.js"></script>



  <script>
  $.getScript("//cdn.jsdelivr.net/npm/leancloud-storage@4.1.0/dist/av-min.js", () => {

    AV.init({
      appId: 'eSrfpdL3XjSFRaFUwnuRl3dn-gzGzoHsz',
      appKey: '6DvkgciWlfleRrT7w9r6snJI',
      serverURLs: 'https://leancloud.cn/',
    });

    const Counter = AV.Object.extend("Counter");
    const Like = AV.Object.extend("Like");

    const showCount = (Counter) => {
      const asyncLimit = new AsyncLimit(2);
      $(".leancloud-counter").each(async (e) => {
        const url = $(".leancloud-counter").eq(e).attr('id').trim();
        const query = new AV.Query("Counter");
        query.equalTo("words", url);
        let count = await asyncLimit.run(() => query.count());
        $(".leancloud-counter").eq(e).text(count ? count : 0);
      });
    }

    const addCount = (Counter) => {
      const url = $(".leancloud-counter").length === 1 ? $(".leancloud-counter").attr('id').trim() : 'https://hxiangdou.github.io';
      var query = new Counter;
      query.save({
        words: url
      });
    }

    const showLike = (Like) => {
      const asyncLimit = new AsyncLimit(2);
      $(".leancloud-like").each(async (e) => {
        const url = $(".leancloud-like").eq(e).attr('id').trim();
        const query = new AV.Query("Like");
        query.equalTo("path", url);
        let count = await asyncLimit.run(() => query.count());
        $(".leancloud-like").eq(e).text(count ? count : 0);
      });
    }

    const addLike = (Like) => {
      const url = $(".leancloud-like").length === 1 ? $(".leancloud-like").attr('id').trim() : 'https://hxiangdou.github.io';
      var query = new Like;
      query.save({
        path: url,
        nickName: 'Anonymous'
      });
      $(".leancloud-like").addClass('islike');
      $(".fab-like").children(".iconfont").removeClass("iconheart").addClass("iconheart-fill").css("color", "#eb3223");
      ZHAOO.zui.message({ text: '爱你哦~', type: 'success' });
      setTimeout(() => showLike(Like), 1000);
    }

    const handleLikeClick = () => {
      const isLike = $(".leancloud-like").length === 1 && $(".leancloud-like").hasClass('islike') ? true : false;
      if (isLike) {
        ZHAOO.zui.message({ text: '小心心不可以收回呢~', type: 'warning' });
      } else {
        addLike(Like);
      }
    }

    $(function () {
      addCount(Counter);
      showCount(Counter);
      showLike(Like);
      $(".fab-like").on("click", function () {
        handleLikeClick();
      });
    });

  });
</script>



  

<script>
  (function (i, s, o, g, r, a, m) {
    i["DaoVoiceObject"] = r;
    i[r] = i[r] || function () {
      (i[r].q = i[r].q || []).push(arguments)
    }, i[r].l = 1 * new Date();
    a = s.createElement(o), m = s.getElementsByTagName(o)[0];
    a.async = 1;
    a.src = g;
    a.charset = "utf-8";
    m.parentNode.insertBefore(a, m)
  })(window, document, "script", "https://widget.daovoice.io/widget/0f81ff2f.js", "daovoice")
  daovoice('init', {
    app_id: "38fcd752"
  }, {
    launcher: {
      disableLauncherIcon: true,
    },
  });
  daovoice('update');
</script>



  <script>
    (function () {
      var bp = document.createElement('script');
      var curProtocol = window.location.protocol.split(':')[0];
      if (curProtocol === 'https') {
        bp.src = 'https://zz.bdstatic.com/linksubmit/push.js';
      } else {
        bp.src = 'http://push.zhanzhang.baidu.com/push.js';
      }
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(bp, s);
    })();
  </script>













</html>